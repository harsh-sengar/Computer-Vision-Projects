{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Vision_transformer_VOC.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh-sengar/Computer-Vision-Projects/blob/main/Vision_transformer_VOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LnJJIMuqE84"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9NxqzV9qE89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgT7b0cpqE8-"
      },
      "source": [
        "cd /content/gdrive/MyDrive/Research/Object Detection/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7BVy4fqEXpI"
      },
      "source": [
        "import xml.etree.ElementTree as et\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "from itertools import chain\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from ipywidgets import IntProgress\n",
        "\n",
        "import json\n",
        "from PIL import Image\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn6JejQfDafh"
      },
      "source": [
        "%%javascript\n",
        "\n",
        "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
        "    if (use_tabs === undefined) {\n",
        "        use_tabs = true; \n",
        "    }\n",
        "\n",
        "    // apply setting to all current CodeMirror instances\n",
        "    IPython.notebook.get_cells().map(\n",
        "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
        "    );\n",
        "    // make sure new CodeMirror instances created in the future also use this setting\n",
        "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
        "\n",
        "    };\n",
        "\n",
        "IPython.tab_as_tab_everywhere()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jlm2dGH-3NX"
      },
      "source": [
        "class VOC2007_dataset(Dataset):\n",
        "  def __init__(self,label_dict,train_files,data_transforms=None):\n",
        "    self.labels = [] # Labels of the object in the image\n",
        "    self.paths = [] # Path of the image\n",
        "\n",
        "    for i in train_files:\n",
        "      tree = et.parse(i)\n",
        "      path = './VOCdevkit/VOC2007/JPEGImages/'+i[i.rfind('/')+1:].split('.')[0]+'.jpg'\n",
        "      self.paths.append(path)\n",
        "      root = tree.getroot()\n",
        "      self.label = [0]*21\n",
        "      \n",
        "      for child in root:\n",
        "        if child.tag=='object':\n",
        "          name = ''\n",
        "          for child1 in child:\n",
        "            if child1.tag=='name':\n",
        "              name = child1.text\n",
        "              self.label[label_dict[name]] = 1\n",
        "      self.label = torch.as_tensor(self.label)\n",
        "      self.labels.append(self.label) \n",
        "\n",
        "    self.n_samples = len(train_files)\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = Image.open(self.paths[index])\n",
        "    if data_transforms:\n",
        "      img = data_transforms(img)\n",
        "    return img,self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkgPNDEQDppU"
      },
      "source": [
        "class VOC2007_dataset_test(Dataset):\n",
        "  def __init__(self,label_dict,data_transforms=None):\n",
        "    self.labels = []\n",
        "    self.paths = []\n",
        "\n",
        "    files = []\n",
        "\n",
        "    for xyz in os.walk('./test/VOCdevkit/VOC2007/Annotations/'):\n",
        "      path = xyz[0]\n",
        "      for val in xyz[2]:\n",
        "        files.append(path+val)\n",
        "        \n",
        "\n",
        "    for i in files:\n",
        "      tree = et.parse(i)\n",
        "      path = './test/VOCdevkit/VOC2007/JPEGImages/'+i[i.rfind('/')+1:].split('.')[0]+'.jpg'\n",
        "      self.paths.append(path)\n",
        "      root = tree.getroot()\n",
        "      self.label = [0]*21\n",
        "\n",
        "      for child in root:\n",
        "        if child.tag=='object':\n",
        "          name = ''\n",
        "          for child1 in child:\n",
        "            if child1.tag=='name':\n",
        "              name = child1.text\n",
        "              self.label[label_dict[name]] = 1\n",
        "      self.label = torch.as_tensor(self.label)  \n",
        "      self.labels.append(self.label)\n",
        "    \n",
        "    self.n_samples = len(files)\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = Image.open(self.paths[index])\n",
        "    if data_transforms:\n",
        "      img = data_transforms(img)\n",
        "\n",
        "    return img,self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yyvEVso70vM"
      },
      "source": [
        "f = open('./VOCdevkit/VOC2007/ImageSets/Main/train.txt')\n",
        "train_files = []\n",
        "for i in f.readlines():\n",
        "    i = i.split('\\n')[0]\n",
        "    train_files.append('./VOCdevkit/VOC2007/Annotations/'+i+'.xml')\n",
        "\n",
        "f1 = open('./VOCdevkit/VOC2007/ImageSets/Main/val.txt')\n",
        "val_files = []\n",
        "for i in f1.readlines():\n",
        "    i = i.split('\\n')[0]\n",
        "    val_files.append('./VOCdevkit/VOC2007/Annotations/'+i+'.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG6M_zCFW73H"
      },
      "source": [
        "train_files_temp = train_files\n",
        "val_files_temp = val_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzm1dyiLXNe8"
      },
      "source": [
        "len(val_files_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPe91ilpqE9D"
      },
      "source": [
        "len(train_files_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Zz-ohJn-_A"
      },
      "source": [
        "data_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                                          std = [0.5, 0.5, 0.5])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDSqwobg-Y8a"
      },
      "source": [
        "labels = {\n",
        " 'aeroplane': 18,\n",
        " 'bicycle': 1,\n",
        " 'bird': 15,\n",
        " 'boat': 6,\n",
        " 'bottle': 16,\n",
        " 'bus': 14,\n",
        " 'car': 12,\n",
        " 'cat': 11,\n",
        " 'chair': 13,\n",
        " 'cow': 19,\n",
        " 'diningtable': 20,\n",
        " 'dog': 5,\n",
        " 'horse': 2,\n",
        " 'motorbike': 10,\n",
        " 'person': 4,\n",
        " 'pottedplant': 7,\n",
        " 'sheep': 8,\n",
        " 'sofa': 9,\n",
        " 'train': 3,\n",
        " 'tvmonitor': 17}\n",
        "dataset_train = VOC2007_dataset(label_dict=labels,train_files = train_files_temp,data_transforms=data_transforms)\n",
        "dataset_val = VOC2007_dataset(label_dict=labels,train_files = val_files_temp,data_transforms = data_transforms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkVdVGHQUHTW"
      },
      "source": [
        "dataset_test = VOC2007_dataset_test(label_dict=labels,data_transforms=data_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wips0rt8-ZFx"
      },
      "source": [
        "first_data = dataset_train[1]\n",
        "print(first_data[1].shape)\n",
        "print(first_data[1])\n",
        "temp = first_data[0].numpy()\n",
        "inp = temp.transpose((1, 2, 0))\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "inp = std * inp + mean\n",
        "inp = np.clip(inp, 0, 1)\n",
        "plt.imshow(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8porlrrtWp_0"
      },
      "source": [
        "first_data = dataset_val[1]\n",
        "print(first_data[0].shape)\n",
        "print(first_data[1])\n",
        "temp = first_data[0].numpy()\n",
        "inp = temp.transpose((1, 2, 0))\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "inp = std * inp + mean\n",
        "inp = np.clip(inp, 0, 1)\n",
        "plt.imshow(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C71O8MIRWqOv"
      },
      "source": [
        "batch_size=32\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
        "                                           shuffle=True,num_workers=8)\n",
        "validation_loader = DataLoader(dataset_val, batch_size=batch_size,\n",
        "                                                shuffle=True,num_workers=8)\n",
        "\n",
        "test_loader = DataLoader(dataset_test,batch_size=batch_size,num_workers=8)\n",
        "dataloaders = {}\n",
        "dataloaders['train'] = train_loader\n",
        "dataloaders['val'] = validation_loader\n",
        "dataloaders['test'] = test_loader\n",
        "dataset_sizes = {'train':len(dataset_train),'val':len(dataset_val),'test':len(dataset_test)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fwf7tO2WqSH"
      },
      "source": [
        "print(dataset_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m1VTn8e667E"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Iu5iGNqE9H"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jthz1khvqE9H"
      },
      "source": [
        "def train_model_fine_tune(model, dataloaders, criterion, optimizer, labels_dict, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            multi_label_acc = 0\n",
        "            pre_label = [0]*21\n",
        "            cat_label = [0]*21\n",
        "            all_label = [0]*21\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    labels = labels.type_as(outputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                outputs = torch.sigmoid(outputs)\n",
        "                predicted = torch.round(outputs)\n",
        "                running_corrects += torch.sum(predicted == labels.data)\n",
        "\n",
        "                predict_and_label = [p*l for p,l in zip(predicted,labels)]\n",
        "                pre_or_label = [l+p for l,p in zip(predicted,labels)] \n",
        "                for pre in pre_or_label:\n",
        "                    pre[pre!=0] = 1\n",
        "                pre_label = [p+c for p,c in zip(pre_label, predict_and_label)]\n",
        "                cat_label = [c+l for c,l in zip(cat_label,labels)]\n",
        "                all_label = [a+l for a,l in zip(all_label,pre_or_label)]\n",
        "                \n",
        "                for a,o in zip(predict_and_label, pre_or_label):\n",
        "                    if o.sum().item() != 0:\n",
        "                        multi_label_acc = multi_label_acc + (a.sum().item()/o.sum().item())\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            \n",
        "            pre_label = [ sum(x) for x in zip(*pre_label) ]\n",
        "            cat_label = [ sum(x) for x in zip(*cat_label) ]\n",
        "            all_label = [ sum(x) for x in zip(*all_label) ]\n",
        "            pre_correct = 0\n",
        "            cat_correct = 0\n",
        "            all_pre_label = 0 \n",
        "            \n",
        "\n",
        "            acc_labels = []\n",
        "            for p,c in zip(pre_label, cat_label):\n",
        "                pre_correct = pre_correct + p.item()\n",
        "                cat_correct = cat_correct + c.item()\n",
        "                if c.item() != 0:\n",
        "                    acc_labels.append(p.item()/c.item())\n",
        "                else:\n",
        "                    acc_labels.append(0)\n",
        "\n",
        "            for a in all_label:\n",
        "                all_pre_label = all_pre_label + a.item()\n",
        "\n",
        "            label_accuracy = {}\n",
        "            label_accuracy =  {k:v for k,v in zip(labels_dict.keys(),acc_labels[1:])}\n",
        "            epoch_acc = (pre_correct/all_pre_label)*100\n",
        "            print(\"Recall\", (pre_correct/cat_correct)*100)\n",
        "            print(\"Multi Label Accuracy\", (multi_label_acc/len(dataloaders[phase].dataset))*100)\n",
        "            print('{} Loss: {:.4f} Precision: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk_lZZtlqE9I"
      },
      "source": [
        "import timm\n",
        "model_timm = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes = 21).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf-lGFXHqE9I"
      },
      "source": [
        "optimizer_ft = optim.Adam(model_timm.parameters(), lr=0.0001)\n",
        "criterion_multi_label = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h4ptqU3YqE9I"
      },
      "source": [
        "model_ft, hist = train_model_fine_tune(model_timm, dataloaders, criterion_multi_label, optimizer_ft, labels, num_epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCy_W5g9qE9I"
      },
      "source": [
        "model_name = timm.list_models(pretrained=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su3u1hU2qE9J"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp73YfiG67Qf"
      },
      "source": [
        "plt.plot(range(80),train_loss,color='blue',label='training')\n",
        "plt.plot(range(80),val_loss,color='green',label = 'validation')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.title('lr=0.01, momentum=0.9) lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.05)')\n",
        "plt.savefig('loss_epoch_curve.png')\n",
        "plt.show()\n",
        "\n",
        "train_acc1 = [float(i) for i in train_acc]\n",
        "val_acc1 = [float(i) for i in val_acc]\n",
        "\n",
        "plt.plot(range(80),train_acc1,color='blue',label='training')\n",
        "plt.plot(range(80),val_acc1,color='green',label = 'validation')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.title('lr=0.01, momentum=0.9) lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.05)')\n",
        "plt.savefig(\"accuracy_epoch_curve.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEGh6ip367U1"
      },
      "source": [
        "phase = 'test'\n",
        "running_loss = 0.0\n",
        "since = time.time()\n",
        "running_corrects = 0\n",
        "for inputs, labels in dataloaders['test']:\n",
        "  inputs = inputs.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "  with torch.set_grad_enabled(phase == 'train'):\n",
        "    outputs = model_ft(inputs)\n",
        "    prob = nn.functional.softmax(outputs)\n",
        "                    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "                    \n",
        "    loss = criterion(outputs, labels)\n",
        "                    \n",
        "    if phase == 'train':\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "  running_loss += loss.item() * inputs.size(0)\n",
        "                \n",
        "  running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "\n",
        "epoch_loss = running_loss / dataset_sizes[phase]\n",
        "epoch_acc = running_corrects.double() / dataset_sizes['test']\n",
        "\n",
        "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "time_elapsed = time.time() - since\n",
        "print('Epoch complete in {:.0f}m {:.0f}s'.format(\n",
        "\n",
        "time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lghRbIKw75ld"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            prob = nn.functional.softmax(outputs)\n",
        "\n",
        "            print(prob.shape)\n",
        "          \n",
        "            prob,indices = torch.sort(prob,descending=True)\n",
        "            prob = prob[:,:3]\n",
        "            print(prob)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            print(inputs.size())\n",
        "            preds = preds.to(\"cpu\").numpy()\n",
        "            labels = labels.to(\"cpu\").numpy()\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                \n",
        "                \n",
        "                ax.set_title('predicted: {} {} {}'.format(class_names[preds[j]],prob[j],\n",
        "                                                        class_names[labels[j]]))\n",
        "                \n",
        "                imshow(inputs[j].to(\"cpu\"))\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TamjpAv75o2"
      },
      "source": [
        "visualize_model(model_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi9BOEqD8DdW"
      },
      "source": [
        "data_transforms = torchvision.transforms.Compose([torchvision.transforms.Resize((224,224))\n",
        "                    ,transforms.ToTensor()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2mc8LgM8mFz"
      },
      "source": [
        "def intersection_union(b,b1):\n",
        "  max_val=0\n",
        "  x1_min = b[0]\n",
        "  y1_min = b[1]\n",
        "  x1_max = b[2]\n",
        "  y1_max = b[3]\n",
        "  \n",
        "  x2_min = b1[:,0]\n",
        "  y2_min = b1[:,1]\n",
        "  x2_max = b1[:,2]\n",
        "  y2_max = b1[:,3]\n",
        "\n",
        "  #print(x1_min,y1_min,x1_max,y1_max,x2_min,y2_min,x2_max,y2_max)\n",
        "  \n",
        "\n",
        "  x1 = np.minimum(x1_max,x2_max)\n",
        "  x2 = np.maximum(x1_min,x2_min)\n",
        "  y1 = np.minimum(y1_max,y2_max)\n",
        "  y2 = np.maximum(y1_min,y2_min)\n",
        "\n",
        "  #print(x1,x2,y1,y2)\n",
        "\n",
        "  x_dist = np.maximum(0,(x1 - x2))\n",
        "  y_dist = np.maximum(0,(y1-y2))\n",
        "\n",
        "  intersect = x_dist * y_dist\n",
        "  \n",
        "  union = (x1_max-x1_min)*(y1_max-y1_min)+(x2_max-x2_min)*(y2_max-y2_min)-intersect\n",
        "\n",
        "  return intersect/union\n",
        "\n",
        "def calc_mAP(predictions,dataset1,iou_thresh,num_classes):\n",
        "\n",
        "  average_precisions = []\n",
        "  for class_c in range(1,num_classes):\n",
        "    filter_predictions = []\n",
        "    filter_true_box = {}\n",
        "    total_ground_truth=0\n",
        "  \n",
        "    for i in predictions:\n",
        "      f = i[0]\n",
        "      pred_class = i[1]\n",
        "      #pred_boxes = i[3]\n",
        "    \n",
        "      for j,lab in enumerate(pred_class):\n",
        "        if labels_dict[lab]==class_c:          \n",
        "          filter_predictions.append([f,class_c,i[2][j],i[3][j]])\n",
        "        \n",
        "      true_class = dataset1.data1[f][1]\n",
        "      true_boxes = dataset1.data1[f][0]\n",
        "      filter_true_box[f] = []\n",
        "\n",
        "      for j in range(len(true_class)):\n",
        "        if true_class[j]==class_c:\n",
        "          filter_true_box[f].append(true_boxes[j])\n",
        "          total_ground_truth+=1\n",
        "      \n",
        "      \n",
        "    TP = torch.zeros(len(filter_predictions))\n",
        "    FP = torch.zeros(len(filter_predictions))\n",
        "    #taken = torch.zeros(len(filter_true_box))\n",
        "    taken = {}\n",
        "    for key in filter_true_box:\n",
        "      taken[key] = torch.zeros(len(filter_true_box[key]))\n",
        "    \n",
        "    \n",
        "\n",
        "    #print('filter_true_box ',filter_true_box,'\\ntotal ground truth ',total_ground_truth)\n",
        "    filter_predictions.sort(key=lambda x:x[2],reverse=True)\n",
        "    \n",
        "    for j,prediction in enumerate(filter_predictions):\n",
        "\n",
        "      f = prediction[0]\n",
        "      true_boxes = np.array(filter_true_box[f]) \n",
        "      #print('true_boxes ',type(true_boxes),true_boxes)\n",
        "      \n",
        "      if true_boxes.shape[0]==0:\n",
        "        FP[j]=1\n",
        "        continue\n",
        "\n",
        "      iou_overlap = intersection_union(prediction[3],true_boxes)\n",
        "\n",
        "      #print(iou_overlap)\n",
        "      max_iou = np.max(iou_overlap,0)\n",
        "      idx = np.argmax(iou_overlap)\n",
        "\n",
        "      if max_iou >= iou_thresh:\n",
        "        \n",
        "        if taken[f][idx]==0:\n",
        "          TP[j]=1\n",
        "          taken[f][idx]=1\n",
        "      \n",
        "        else:\n",
        "          FP[j] = 1\n",
        "\n",
        "      else:\n",
        "        FP[j] = 1\n",
        "      \n",
        "    TP_cumsum = torch.cumsum(TP,dim=0)\n",
        "    FP_cumsum = torch.cumsum(FP,dim=0)\n",
        "    recalls = TP_cumsum/total_ground_truth\n",
        "    precisions = TP_cumsum/(TP_cumsum+FP_cumsum)\n",
        "    precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "    recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "    average_precisions.append(torch.trapz(precisions,recalls))\n",
        "    #print(recalls,'\\n', precisions)\n",
        "    plt.plot(recalls,precisions)\n",
        "    #plt.xticks(np.arange(0,1.1,0.1))\n",
        "    #plt.yticks(np.arange(0,1.1,0.1))\n",
        "    plt.xlabel('recalls')\n",
        "    plt.ylabel('precisions')\n",
        "    plt.title(class_names[class_c])\n",
        "    plt.savefig('{}.png'.format(class_names[class_c]))\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "\n",
        "  print(sum(average_precisions)/len(average_precisions))\n",
        "\n",
        "\n",
        "calc_mAP(filter_detections,dataset_val,0.3,21)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aNpiUxx8vrG"
      },
      "source": [
        "Image.open('./VOCdevkit/VOC2007/JPEGImages/000005.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0iNSPzK8vv_"
      },
      "source": [
        "img = Image.open('./VOCdevkit/VOC2007/JPEGImages/006524.jpg')\n",
        "data_transforms = torchvision.transforms.Compose([torchvision.transforms.Resize((256)),\n",
        "                                                  transforms.RandomVerticalFlip(),\n",
        "                                                  transforms.RandomHorizontalFlip(),\n",
        "                                                  transforms.RandomAffine(degrees=0,translate=(0.25,0.25))])\n",
        "\n",
        "#temp = data_transforms(img)\n",
        "w,h = img.size\n",
        "diff=0\n",
        "diff1=0\n",
        "w1=0\n",
        "w2=0\n",
        "h1=0\n",
        "h2=0\n",
        "a=0\n",
        "b=0\n",
        "while diff<64:\n",
        "  a = np.random.random()\n",
        "  a1 = int(a*w)\n",
        "  b = np.random.random()\n",
        "  b1 = int(b*w)\n",
        "  w1 = np.minimum(a1,b1)\n",
        "  w2 = np.maximum(a1,b1)\n",
        "  diff = w2-w1\n",
        "#print(w1,w2,a,b)\n",
        "while diff1<64:\n",
        "  a = np.random.random()\n",
        "  a1 = int(a*h)\n",
        "  b = np.random.random()\n",
        "  b1 = int(b*h)\n",
        "  h1 = np.minimum(a1,b1)\n",
        "  h2 = np.maximum(a1,b1)\n",
        "  diff1 = h2-h1\n",
        "#print(h1,h2,a,b)\n",
        "img.crop((w1,h1,w2,h2))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}